Attaching to webdemo-appsqldb-1, webdemo-backend_stable_v1_1-1, webdemo-backend_stable_v2_1-1, webdemo-donut_1-1, webdemo-frontend-1, webdemo-gfpgan_1-1, webdemo-minio-1, webdemo-minio_upload-1, webdemo-req_db-1
webdemo-req_db-1               | INFO:     Started server process [1]
webdemo-req_db-1               | INFO:     Waiting for application startup.
webdemo-req_db-1               | INFO:     Application startup complete.
webdemo-req_db-1               | INFO:     Uvicorn running on http://0.0.0.0:8509 (Press CTRL+C to quit)
webdemo-appsqldb-1             | 2023-02-23 09:46:02+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.32-1.el8 started.
webdemo-appsqldb-1             | 2023-02-23 09:46:03+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  | ==========
webdemo-backend_stable_v2_1-1  | == CUDA ==
webdemo-backend_stable_v2_1-1  | ==========
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  | CUDA Version 11.3.1
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  | Container image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
webdemo-backend_stable_v2_1-1  | By pulling and using the container, you accept the terms and conditions of this license:
webdemo-backend_stable_v2_1-1  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
webdemo-appsqldb-1             | 2023-02-23 09:46:03+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.32-1.el8 started.
webdemo-backend_stable_v2_1-1  | 
webdemo-donut_1-1              | 
webdemo-donut_1-1              | ==========
webdemo-donut_1-1              | == CUDA ==
webdemo-backend_stable_v1_1-1  | 
webdemo-backend_stable_v1_1-1  | ==========
webdemo-backend_stable_v1_1-1  | == CUDA ==
webdemo-backend_stable_v1_1-1  | ==========
webdemo-donut_1-1              | ==========
webdemo-backend_stable_v1_1-1  | 
webdemo-backend_stable_v1_1-1  | CUDA Version 11.3.1
webdemo-backend_stable_v1_1-1  | 
webdemo-backend_stable_v1_1-1  | Container image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
webdemo-donut_1-1              | 
webdemo-donut_1-1              | CUDA Version 11.3.1
webdemo-backend_stable_v1_1-1  | 
webdemo-backend_stable_v1_1-1  | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
webdemo-backend_stable_v1_1-1  | By pulling and using the container, you accept the terms and conditions of this license:
webdemo-backend_stable_v1_1-1  | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
webdemo-backend_stable_v1_1-1  | 
webdemo-backend_stable_v1_1-1  | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
webdemo-donut_1-1              | 
webdemo-donut_1-1              | Container image Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
webdemo-donut_1-1              | 
webdemo-donut_1-1              | This container image and its contents are governed by the NVIDIA Deep Learning Container License.
webdemo-donut_1-1              | By pulling and using the container, you accept the terms and conditions of this license:
webdemo-donut_1-1              | https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
webdemo-donut_1-1              | 
webdemo-donut_1-1              | A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
webdemo-minio-1                | MinIO Object Storage Server
webdemo-minio-1                | Copyright: 2015-2023 MinIO, Inc.
webdemo-minio-1                | License: GNU AGPLv3 <https://www.gnu.org/licenses/agpl-3.0.html>
webdemo-minio-1                | Version: RELEASE.2023-02-17T17-52-43Z (go1.19.6 linux/amd64)
webdemo-minio-1                | 
webdemo-minio-1                | Status:         1 Online, 0 Offline. 
webdemo-minio-1                | API: http://192.168.32.5:9000  http://127.0.0.1:9000 
webdemo-minio-1                | Console: http://192.168.32.5:9001 http://127.0.0.1:9001 
webdemo-minio-1                | 
webdemo-minio-1                | Documentation: https://min.io/docs/minio/linux/index.html
webdemo-minio-1                | Warning: The standard parity is set to 0. This can lead to data loss.
webdemo-backend_stable_v1_1-1  | 
webdemo-donut_1-1              | 
webdemo-appsqldb-1             | '/var/lib/mysql/mysql.sock' -> '/var/run/mysqld/mysqld.sock'
webdemo-minio_upload-1         | Bucket 'modelweight' already exists
webdemo-minio_upload-1         | Bucket 'config_data' already exists
webdemo-minio-1                | 
webdemo-minio-1                |  You are running an older version of MinIO released 5 days ago 
webdemo-minio-1                |  Update: Run `mc admin update` 
webdemo-minio-1                | 
webdemo-minio-1                | 
webdemo-appsqldb-1             | 2023-02-23T09:46:03.541193Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
webdemo-appsqldb-1             | 2023-02-23T09:46:03.542790Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.32) starting as process 1
webdemo-appsqldb-1             | 2023-02-23T09:46:03.548904Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
webdemo-minio_upload-1 exited with code 0
webdemo-appsqldb-1             | 2023-02-23T09:46:03.875071Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
webdemo-appsqldb-1             | 2023-02-23T09:46:04.168324Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
webdemo-appsqldb-1             | 2023-02-23T09:46:04.168361Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
webdemo-appsqldb-1             | 2023-02-23T09:46:04.175191Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
webdemo-appsqldb-1             | 2023-02-23T09:46:04.193595Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
webdemo-appsqldb-1             | 2023-02-23T09:46:04.193707Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
webdemo-frontend-1             | 
webdemo-frontend-1             | Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.
webdemo-frontend-1             | 
webdemo-frontend-1             | 
webdemo-frontend-1             |   You can now view your Streamlit app in your browser.
webdemo-frontend-1             | 
webdemo-frontend-1             |   Network URL: http://192.168.32.10:8501
webdemo-frontend-1             |   External URL: http://185.47.227.172:8501
webdemo-frontend-1             | 
webdemo-backend_stable_v2_1-1  | /opt/conda/envs/myenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/envs/myenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE
webdemo-backend_stable_v2_1-1  |   warn(f"Failed to load image Python extension: {e}")
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:46:06.804 WARNING xformers: A matching Triton is not available, some optimizations will not be enabled.
webdemo-backend_stable_v2_1-1  | Error caught was: module 'triton.language' has no attribute 'constexpr'
webdemo-req_db-1               | INFO:     192.168.32.7:56662 - "POST /initdb HTTP/1.1" 200 OK
webdemo-req_db-1               | INFO:     192.168.32.7:56672 - "POST /initdb HTTP/1.1" 200 OK
webdemo-donut_1-1              | INFO:     Started server process [1]
webdemo-donut_1-1              | INFO:     Waiting for application startup.
webdemo-donut_1-1              | INFO:     Application startup complete.
webdemo-donut_1-1              | INFO:     Uvicorn running on http://0.0.0.0:8503 (Press CTRL+C to quit)
webdemo-gfpgan_1-1             | INFO:     Started server process [1]
webdemo-gfpgan_1-1             | INFO:     Waiting for application startup.
webdemo-gfpgan_1-1             | INFO:     Application startup complete.
webdemo-gfpgan_1-1             | INFO:     Uvicorn running on http://0.0.0.0:8506 (Press CTRL+C to quit)
webdemo-backend_stable_v1_1-1  | /opt/conda/envs/ldm/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.
webdemo-backend_stable_v1_1-1  |   warnings.warn(
webdemo-backend_stable_v1_1-1  | Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'logit_scale', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias']
webdemo-backend_stable_v1_1-1  | - This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
webdemo-backend_stable_v1_1-1  | - This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
webdemo-req_db-1               | INFO:     192.168.32.8:49392 - "POST /initdb HTTP/1.1" 200 OK
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:38.745 INFO    torch.distributed.nn.jit.instantiator: Created a temporary directory at /tmp/tmpueii6wf2
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:38.745 INFO    torch.distributed.nn.jit.instantiator: Writing /tmp/tmpueii6wf2/_remote_module_non_scriptable.py
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:38.854 INFO    root: Loading ViT-H-14 model config.
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:46.710 INFO    root: Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:55.212 
webdemo-backend_stable_v2_1-1  |   [33m[1mWarning:[0m to view this Streamlit app on a browser, run it with the following
webdemo-backend_stable_v2_1-1  |   command:
webdemo-backend_stable_v2_1-1  | 
webdemo-backend_stable_v2_1-1  |     streamlit run backend.py [ARGUMENTS]
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:47:58.512 Loading ViT-H-14 model config.
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:48:06.477 Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
webdemo-backend_stable_v1_1-1  | Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'logit_scale', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias']
webdemo-backend_stable_v1_1-1  | - This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
webdemo-backend_stable_v1_1-1  | - This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
webdemo-req_db-1               | INFO:     192.168.32.8:57254 - "POST /initdb HTTP/1.1" 200 OK
webdemo-backend_stable_v1_1-1  | INFO:     Started server process [35]
webdemo-backend_stable_v1_1-1  | INFO:     Waiting for application startup.
webdemo-backend_stable_v1_1-1  | INFO:     Application startup complete.
webdemo-backend_stable_v1_1-1  | INFO:     Uvicorn running on http://0.0.0.0:8504 (Press CTRL+C to quit)
webdemo-backend_stable_v2_1-1  | uploading model weights from MinIO bucket
webdemo-backend_stable_v2_1-1  | model successfully downloaded!
webdemo-backend_stable_v2_1-1  | Loading model from model_weight_gen
webdemo-backend_stable_v2_1-1  | Global Step: 140000
webdemo-backend_stable_v2_1-1  | LatentDiffusion: Running in v-prediction mode
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and using 20 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | DiffusionWrapper has 865.91 M params.
webdemo-backend_stable_v2_1-1  | making attention of type 'vanilla-xformers' with 512 in_channels
webdemo-backend_stable_v2_1-1  | building MemoryEfficientAttnBlock with 512 in_channels...
webdemo-backend_stable_v2_1-1  | Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
webdemo-backend_stable_v2_1-1  | making attention of type 'vanilla-xformers' with 512 in_channels
webdemo-backend_stable_v2_1-1  | building MemoryEfficientAttnBlock with 512 in_channels...
webdemo-backend_stable_v2_1-1  | LatentUpscaleDiffusion: Running in v-prediction mode
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is None and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 1024, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 512, context_dim is 1024 and using 8 heads.
webdemo-backend_stable_v2_1-1  | DiffusionWrapper has 473.40 M params.
webdemo-backend_stable_v2_1-1  | making attention of type 'vanilla-xformers' with 512 in_channels
webdemo-backend_stable_v2_1-1  | building MemoryEfficientAttnBlock with 512 in_channels...
webdemo-backend_stable_v2_1-1  | Working with z of shape (1, 4, 64, 64) = 16384 dimensions.
webdemo-backend_stable_v2_1-1  | making attention of type 'vanilla-xformers' with 512 in_channels
webdemo-backend_stable_v2_1-1  | building MemoryEfficientAttnBlock with 512 in_channels...
webdemo-backend_stable_v2_1-1  | LatentInpaintDiffusion: Running in eps-prediction mode
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and using 5 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.
webdemo-backend_stable_v2_1-1  | Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and using 10 heads.
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:48:30.833 Loading ViT-H-14 model config.
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:48:37.578 Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
webdemo-req_db-1               | INFO:     192.168.32.6:51188 - "POST /initdb HTTP/1.1" 200 OK
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:50:07.551 Loading ViT-H-14 model config.
webdemo-backend_stable_v2_1-1  | 2023-02-23 09:50:14.552 Loading pretrained ViT-H-14 weights (laion2b_s32b_b79k).
webdemo-req_db-1               | INFO:     192.168.32.6:53282 - "POST /initdb HTTP/1.1" 200 OK
webdemo-backend_stable_v2_1-1  | INFO:     Started server process [35]
webdemo-backend_stable_v2_1-1  | INFO:     Waiting for application startup.
webdemo-backend_stable_v2_1-1  | INFO:     Application startup complete.
webdemo-backend_stable_v2_1-1  | INFO:     Uvicorn running on http://0.0.0.0:8505 (Press CTRL+C to quit)
webdemo-gfpgan_1-1             | database initialization failed
webdemo-gfpgan_1-1             | database initialization failed
webdemo-gfpgan_1-1             | INFO:     87.236.176.83:56825 - "GET / HTTP/1.1" 200 OK
webdemo-req_db-1               | WARNING:  Invalid HTTP request received.
webdemo-backend_stable_v1_1-1  | WARNING:  Invalid HTTP request received.
webdemo-gfpgan_1-1             | WARNING:  Invalid HTTP request received.
webdemo-backend_stable_v2_1-1  | WARNING:  Invalid HTTP request received.
webdemo-donut_1-1              | WARNING:  Invalid HTTP request received.
webdemo-donut_1-1              | INFO:     162.142.125.210:34444 - "GET / HTTP/1.1" 200 OK
webdemo-donut_1-1              | INFO:     162.142.125.210:47446 - "PRI %2A HTTP/2.0" 404 Not Found
webdemo-donut_1-1              | WARNING:  Invalid HTTP request received.
webdemo-donut_1-1              | INFO:     162.142.125.210:56138 - "GET /favicon.ico HTTP/1.1" 404 Not Found
webdemo-backend_stable_v1_1-1  | Downloading model from MinIO bucket
webdemo-backend_stable_v1_1-1  | Loading model from model_weight
webdemo-backend_stable_v1_1-1  | Global Step: 470000
webdemo-backend_stable_v1_1-1  | LatentDiffusion: Running in eps-prediction mode
webdemo-backend_stable_v1_1-1  | DiffusionWrapper has 859.52 M params.
webdemo-backend_stable_v1_1-1  | making attention of type 'vanilla' with 512 in_channels
webdemo-backend_stable_v1_1-1  | Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
webdemo-backend_stable_v1_1-1  | making attention of type 'vanilla' with 512 in_channels
webdemo-backend_stable_v1_1-1  | Downloading model from MinIO bucket
webdemo-backend_stable_v1_1-1  | Loading model from model_weight
webdemo-backend_stable_v1_1-1  | Global Step: 470000
webdemo-backend_stable_v1_1-1  | LatentDiffusion: Running in eps-prediction mode
webdemo-backend_stable_v1_1-1  | DiffusionWrapper has 859.52 M params.
webdemo-backend_stable_v1_1-1  | making attention of type 'vanilla' with 512 in_channels
webdemo-backend_stable_v1_1-1  | Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
webdemo-backend_stable_v1_1-1  | making attention of type 'vanilla' with 512 in_channels
webdemo-backend_stable_v1_1-1  | INFO:     167.248.133.117:36398 - "GET / HTTP/1.1" 200 OK
webdemo-backend_stable_v1_1-1  | INFO:     167.248.133.117:46922 - "PRI %2A HTTP/2.0" 404 Not Found
webdemo-backend_stable_v1_1-1  | WARNING:  Invalid HTTP request received.
webdemo-backend_stable_v1_1-1  | INFO:     167.248.133.117:52898 - "GET /favicon.ico HTTP/1.1" 404 Not Found
webdemo-gfpgan_1-1             | WARNING:  Invalid HTTP request received.
webdemo-gfpgan_1-1             | INFO:     162.142.125.210:35296 - "GET / HTTP/1.1" 200 OK
webdemo-gfpgan_1-1             | INFO:     162.142.125.210:59230 - "GET / HTTP/1.1" 200 OK
webdemo-gfpgan_1-1             | INFO:     162.142.125.210:39298 - "PRI %2A HTTP/2.0" 404 Not Found
webdemo-gfpgan_1-1             | WARNING:  Invalid HTTP request received.
webdemo-gfpgan_1-1             | INFO:     162.142.125.210:47996 - "GET /favicon.ico HTTP/1.1" 404 Not Found
webdemo-frontend-1             | 2023-02-23 17:22:51.158 Uncaught app exception
webdemo-frontend-1             | Traceback (most recent call last):
webdemo-frontend-1             |   File "/usr/local/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 565, in _run_script
webdemo-frontend-1             |     exec(code, module.__dict__)
webdemo-frontend-1             |   File "/home/HOME.py", line 34, in <module>
webdemo-frontend-1             |     port_config = load_config_port()
webdemo-frontend-1             |   File "/home/HOME.py", line 27, in load_config_port
webdemo-frontend-1             |     client.fget_object("configdata", "storage/config.yaml", "config_file")
webdemo-frontend-1             |   File "/usr/local/lib/python3.10/site-packages/minio/api.py", line 1077, in fget_object
webdemo-frontend-1             |     os.rename(tmp_file_path, file_path)
webdemo-frontend-1             | FileNotFoundError: [Errno 2] No such file or directory: 'config_file.f722ed159b3b5aeaf20435457468948b.part.minio' -> 'config_file'
